{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83f797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def load_mnist_38(train=True, device=\"cpu\", seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    ds = datasets.MNIST(\n",
    "        root=\"./data\", train=train, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    X, y = [], []\n",
    "    for img, label in ds:\n",
    "        if label in (3, 8):\n",
    "            X.append(img.view(-1).numpy())         # 784 vector in [0,1]\n",
    "            y.append(1 if label == 8 else 0)        # map 8->1, 3->0\n",
    "    X = np.stack(X).astype(np.float64)\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    return X, y\n",
    "\n",
    "def stratified_uniform_split(X, y, num_nodes=4, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx0 = np.where(y == 0)[0]\n",
    "    idx1 = np.where(y == 1)[0]\n",
    "    rng.shuffle(idx0)\n",
    "    rng.shuffle(idx1)\n",
    "    parts = [[] for _ in range(num_nodes)]\n",
    "\n",
    "    for k, idx in enumerate(idx0):\n",
    "        parts[k % num_nodes].append(idx)\n",
    "    for k, idx in enumerate(idx1):\n",
    "        parts[k % num_nodes].append(idx)\n",
    "    node_splits = []\n",
    "    for p in parts:\n",
    "        p = np.array(p, dtype=int)\n",
    "        node_splits.append((X[p], y[p]))\n",
    "    return node_splits\n",
    "\n",
    "#Log regression\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_loss_and_grad(theta, X, y, l2=0.0):\n",
    "    \"\"\"\n",
    "    theta = [w; b], w in R^d, b scalar\n",
    "    Loss: mean cross-entropy; optional L2 on w only.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    w = theta[:-1]\n",
    "    b = theta[-1]\n",
    "    z = X @ w + b\n",
    "    p = sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    #loss over full dataset\n",
    "    loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
    "    #gradient\n",
    "    diff = (p - y)\n",
    "    grad_w = (X.T @ diff) / n + l2 * w\n",
    "    grad_b = np.mean(diff)  # no L2 on bias\n",
    "    grad = np.concatenate([grad_w, np.array([grad_b])])\n",
    "    return loss + (l2 / 2.0) * np.sum(w * w), grad\n",
    "\n",
    "#Graphs\n",
    "def laplacian_from_adjacency(A):\n",
    "    deg = np.diag(np.sum(A, axis=1))\n",
    "    return deg - A\n",
    "\n",
    "def dgd_train(node_splits, A, T=200, alpha=0.15, gamma=0.2, l2=0.0, seed=0):\n",
    "    \"\"\"\n",
    "    node_splits: list of (X_i, y_i) for i=0..3\n",
    "    A: adjacency (4x4) with 1 for edges (i!=j), 0 otherwise (undirected)\n",
    "    Returns: history of global loss (evaluated with node 0's model)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_nodes = len(node_splits)\n",
    "    d = node_splits[0][0].shape[1]\n",
    "    #init theta_i^0 (w,b)\n",
    "    Theta = np.zeros((n_nodes, d + 1), dtype=np.float64)\n",
    "    X_all = np.concatenate([Xi for Xi, yi in node_splits], axis=0)\n",
    "    y_all = np.concatenate([yi for Xi, yi in node_splits], axis=0)\n",
    "\n",
    "    L = laplacian_from_adjacency(A)\n",
    "    deg_max = np.max(np.diag(L))\n",
    "    #alpha < 1/deg_max to keep gossip stable\n",
    "    if alpha >= 1.0 / max(1, deg_max):\n",
    "        print(f\"[warn] alpha={alpha} may be too large for deg_max={deg_max}. \"\n",
    "              f\"Consider using alpha < {1.0/deg_max:.3f}\")\n",
    "\n",
    "    global_loss_hist = []\n",
    "    for t in range(T):\n",
    "        #gossip (vectorized: Theta_half = Theta - alpha * L @ Theta)\n",
    "        Theta_half = Theta - alpha * (L @ Theta)\n",
    "\n",
    "        #local gradient step\n",
    "        Theta_next = np.empty_like(Theta)\n",
    "        for i in range(n_nodes):\n",
    "            Xi, yi = node_splits[i]\n",
    "            _, grad_i = logistic_loss_and_grad(Theta_half[i], Xi, yi, l2=l2)\n",
    "            Theta_next[i] = Theta_half[i] - gamma * grad_i\n",
    "\n",
    "        Theta = Theta_next\n",
    "\n",
    "        # Evaluate global training loss at node 1's model (index 0)\n",
    "        loss_global, _ = logistic_loss_and_grad(Theta[0], X_all, y_all, l2=0.0)\n",
    "        global_loss_hist.append(loss_global)\n",
    "\n",
    "    return np.array(global_loss_hist), Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path 1-2-3-4\n",
    "A_path = np.array([\n",
    "    [0,1,0,0],\n",
    "    [1,0,1,0],\n",
    "    [0,1,0,1],\n",
    "    [0,0,1,0],\n",
    "], dtype=float)\n",
    "\n",
    "#ring 1-2-3-4-1\n",
    "A_ring = np.array([\n",
    "    [0,1,0,1],\n",
    "    [1,0,1,0],\n",
    "    [0,1,0,1],\n",
    "    [1,0,1,0],\n",
    "], dtype=float)\n",
    "\n",
    "#complete graph K4\n",
    "A_complete = np.ones((4,4), dtype=float) - np.eye(4)\n",
    "\n",
    "def run_all(topologies: Dict[str, np.ndarray],\n",
    "            T=250, alpha=None, gamma=0.2, l2=0.0, seed=0):\n",
    "    X, y = load_mnist_38(train=True, seed=seed)\n",
    "    splits = stratified_uniform_split(X, y, num_nodes=4, seed=seed)\n",
    "    losses = {}\n",
    "    for name, A in topologies.items():\n",
    "        deg_max = int(np.max(np.sum(A, axis=1)))\n",
    "        a = alpha if alpha is not None else 0.45 / max(1, deg_max)\n",
    "        hist, _ = dgd_train(splits, A, T=T, alpha=a, gamma=gamma, l2=l2, seed=seed)\n",
    "        losses[name] = hist\n",
    "    return losses\n",
    "\n",
    "topologies = {\"A (path)\": A_path, \"B (ring)\": A_ring, \"C (complete)\": A_complete}\n",
    "losses = run_all(topologies, T=250, gamma=0.2, l2=0.0, seed=1)\n",
    "# Plot\n",
    "plt.figure(figsize=(7,4.5))\n",
    "for name, hist in losses.items():\n",
    "    plt.plot(hist, label=name)\n",
    "plt.xlabel(\"Iteration t\")\n",
    "plt.ylabel(\"Global training loss (node 1 model)\")\n",
    "plt.title(\"DGD: global training loss at node 1 vs. iteration\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e87f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
